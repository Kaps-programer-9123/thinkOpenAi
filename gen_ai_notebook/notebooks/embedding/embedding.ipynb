{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bb4b45",
   "metadata": {},
   "source": [
    "<h2> Best Open-Source Embedding Models (Local Use) </h2>\n",
    "\n",
    "| **Model**                | **Size**          | **Use Case**                          | **Library**               |\n",
    "| ------------------------ | ----------------- | ------------------------------------- | ------------------------- |\n",
    "| `all-MiniLM-L6-v2`       | \\~80MB            | General-purpose sentence embeddings   | SentenceTransformers      |\n",
    "| `E5-base` / `E5-small`   | \\~200MB / \\~100MB | Search, semantic similarity           | Hugging Face Transformers |\n",
    "| `Instructor XL`          | \\~1.5GB           | Task-specific embeddings with prompts | Hugging Face              |\n",
    "| `BGE-M3` / `BGE-base-en` | \\~400MB           | Versatile, multilingual embeddings    | Hugging Face              |\n",
    "| `mpnet-base-v2`          | \\~420MB           | Semantic search & clustering          | SentenceTransformers      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e7c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode sentences\n",
    "sentences = [\"This is a test.\", \"Embeddings are useful.\"]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d8e3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reinforcement learning is...', 'Supervised learning is...']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "query = \"How does reinforcement learning work?\"\n",
    "candidates = [\"Reinforcement learning is...\", \"Supervised learning is...\"]\n",
    "\n",
    "scores = model.predict([(query, doc) for doc in candidates])\n",
    "top_docs = [doc for _, doc in sorted(zip(scores, candidates), reverse=True)]\n",
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2a37b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "docs = [\"Paris is the capital of France.\", \"Berlin is in Germany.\", \"Tokyo is in Japan.\", \"Tokyo is capital of Japan.\",\n",
    "        \"kapil is gen ai engineer working for testing model\",\"kapil works in gen Ai area\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dceb645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[4 5]\n",
      "[[0.67529976 1.7267113 ]]\n",
      "kapil is testing model\n",
      "kapil works in gen Ai area\n"
     ]
    }
   ],
   "source": [
    "docs_embedding = model.encode(docs)\n",
    "\n",
    "dimension = docs_embedding.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "index.add(docs_embedding)\n",
    "\n",
    "query = \"who is testing model\"\n",
    "q_embedding = model.encode([query])\n",
    "print(q_embedding.size)\n",
    "\n",
    "distances, indices = index.search(q_embedding, 2)\n",
    "\n",
    "print(indices[0])\n",
    "print(distances)\n",
    "for i in indices[0]:\n",
    "    print(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26303976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo is capital of Japan.\n",
      "Tokyo is in Japan.\n"
     ]
    }
   ],
   "source": [
    "import faiss #pip install faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model and encode documents\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "docs = [\"Paris is the capital of France.\", \"Berlin is in Germany.\", \"Tokyo is in Japan.\", \"Tokyo is capital of Japan.\",\n",
    "        \"kapil is testing model\",\"kapil works in gen Ai area\"]\n",
    "doc_embeddings = model.encode(docs)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 = Euclidean distance (can use cosine too)\n",
    "\n",
    "index.add(doc_embeddings)  # Store embeddings\n",
    "\n",
    "# Encode query and search\n",
    "query = \"Capital of Japan\"\n",
    "query_embedding = model.encode([query])\n",
    "top_k = 2\n",
    "distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "# Show results\n",
    "for i in indices[0]:\n",
    "    print(docs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b49ba4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript runs in the browser. Score: 0.35208036468145\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "qdrant = QdrantClient(\":memory:\")  # In-memory, or use host='localhost'\n",
    "\n",
    "# Create collection\n",
    "qdrant.create_collection(\n",
    "    collection_name=\"docs\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "texts = [\"Python is great.\", \"JavaScript runs in the browser.\", \"C++ is powerful.\"]\n",
    "embeds = model.encode(texts)\n",
    "\n",
    "points = [\n",
    "    PointStruct(id=i, vector=embeds[i], payload={\"text\": texts[i]})\n",
    "    for i in range(len(texts))\n",
    "]\n",
    "\n",
    "qdrant.upsert(collection_name=\"docs\", points=points)\n",
    "\n",
    "# Query\n",
    "query = \"language used in frontend\"\n",
    "query_vec = model.encode(query)\n",
    "results = qdrant.query_points(collection_name=\"docs\", query=query_vec, limit=2)\n",
    "\n",
    "for r in results:\n",
    "    # print(r.payload[\"text\"], \"Score:\", r.score)\n",
    "    print(r[1][0].payload['text'],  \"Score:\", r[1][0].score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fffb4f",
   "metadata": {},
   "source": [
    "Implementation Steps for Reranking <br>\n",
    "Using Cross-Encoder for Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2f24610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.500707  -10.9921875  -6.6122394  -1.109186 ]\n",
      "[0 3]\n",
      "query is - What is the capital of France\n",
      "Top-K Reranked Documents:\n",
      "Paris is the capital of France.\n",
      "France is a country in Europe.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers  import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load cross-encoder model (this will perform query-document reranking)\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Example query and retrieved documents\n",
    "query = \"What is the capital of France?\"\n",
    "retrieved_docs = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"i am computer programmer\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"France is a country in Europe.\"\n",
    "]\n",
    "\n",
    "# Create query-document pairs (query + each retrieved document)\n",
    "pairs = [(query, doc) for doc in retrieved_docs]\n",
    "\n",
    "# Use the cross-encoder to get relevance scores for each pair\n",
    "scores = model.predict(pairs)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "# Sort documents based on the relevance score in descending order\n",
    "sorted_indices = np.argsort(scores)[::-1]  # Sort indices based on scores in descending order\n",
    "\n",
    "print(sorted_indices[:2])\n",
    "\n",
    "# Get the top-N documents based on reranking\n",
    "top_k_docs = [retrieved_docs[i] for i in sorted_indices[:2]]  # Top 2 most relevant documents\n",
    "\n",
    "# Display reranked documents\n",
    "print(\"query is - What is the capital of France\")\n",
    "print(\"Top-K Reranked Documents:\")\n",
    "for doc in top_k_docs:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84630761",
   "metadata": {},
   "source": [
    "<h2>persist the FAISS index to disk</h2>\n",
    "\n",
    "Save index and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1cb45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: Encode and create index\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "docs = [\"Paris is the capital of France.\", \"Berlin is in Germany.\",\n",
    "        \"Tokyo is in Japan.\", \"Tokyo is capital of Japan.\",\n",
    "        \"kapil is testing model\", \"kapil works in gen Ai area\"]\n",
    "\n",
    "doc_embeddings = model.encode(docs)\n",
    "dimension = doc_embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Step 2: Save index and docs\n",
    "faiss.write_index(index, \"../../data/faiss_index.index\")\n",
    "with open(\"docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(docs, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534de87",
   "metadata": {},
   "source": [
    "Load and Use for Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6945e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo is capital of Japan.\n",
      "Tokyo is in Japan.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load index and docs\n",
    "index = faiss.read_index(\"../../data/faiss_index.index\")\n",
    "with open(\"docs.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "# Load model and query\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "query = \"Capital of Japan\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Search\n",
    "top_k = 2\n",
    "distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "# Show results\n",
    "for i in indices[0]:\n",
    "    print(docs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2d247",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "[Document Source]\n",
    "      ↓\n",
    "[Chunking + Cleaning]\n",
    "      ↓\n",
    "[Embedding (e.g. MiniLM, E5)]\n",
    "      ↓\n",
    "[Vector Store (FAISS, Qdrant, etc.)]\n",
    "      ↓\n",
    "[User Query → Query Embedding]\n",
    "      ↓\n",
    "[Top-k Retrieval]\n",
    "      ↓\n",
    "[Reranking (optional)]\n",
    "      ↓\n",
    "[Prompt Building]\n",
    "      ↓\n",
    "[LLM Generation (GPT, Mistral, etc.)]\n",
    "      ↓\n",
    "[Answer Output / API Response]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3eb005",
   "metadata": {},
   "source": [
    "4. RAG in Production <br>\n",
    "This is where things get serious: scale, latency, monitoring, reliability. <br>\n",
    "\n",
    "    🚀 Key Production Concerns: <br>\n",
    "    Area\tWhat to Watch <br>\n",
    "    Latency\tRetrieval + LLM must stay under response time threshold <br>\n",
    "    Caching\tCache repeated queries and embeddings to reduce compute <br>\n",
    "    Failover\tHandle vector DB / model downtime gracefully <br>\n",
    "    Security\tSanitize inputs, prevent prompt injection <br>\n",
    "    Monitoring\tLog retrieval results, LLM inputs/outputs, and usage <br>\n",
    "    Data versioning\tTrack vector updates when documents change <br>\n",
    "    Cost control\tWatch LLM/API usage (OpenAI, Anthropic) or self-host <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
