{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c6e098",
   "metadata": {},
   "source": [
    "reranking - cross-encoder/ms-marco-MiniLM-L-6-v2\n",
    "\n",
    "Combine dense and sparse retrieval (e.g., BM25 + vector search)\n",
    "\n",
    "\n",
    "Sure â€” here's a list of different RAG types:\n",
    "\n",
    "1. **Standard (Vanilla) RAG**\n",
    "2. **RAG with Cross-Encoder Re-ranking**\n",
    "3. **Hybrid RAG (Dense + Sparse Retrieval)**\n",
    "4. **Multi-hop RAG (Reasoning over multiple documents)**\n",
    "5. **Conversational RAG**\n",
    "6. **Agentic RAG (RAG with tool-using agents)**\n",
    "7. **Self-Reranking RAG (LLM-based re-ranking)**\n",
    "8. **Fusion-in-Decoder RAG (FiD-RAG)**\n",
    "9. **Query-Condensing RAG (with query rewriting)**\n",
    "10. **Document-Graph RAG (RAG with knowledge graphs)**\n",
    "11. **Memory-Augmented RAG (RAG with long-term memory)**\n",
    "12. **Streaming RAG (for real-time or chunked input)**\n",
    "\n",
    "Want a diagram or examples for any of these?\n",
    "\n",
    "\n",
    "\n",
    "Sure â€” here's a list of chain types in LangChain:\n",
    "\n",
    "1. **LLMChain**\n",
    "2. **SimpleSequentialChain**\n",
    "3. **SequentialChain**\n",
    "4. **RetrievalQA**\n",
    "5. **ConversationalRetrievalChain**\n",
    "6. **MultiRetrievalQAChain**\n",
    "7. **RefineDocumentsChain**\n",
    "8. **MapReduceDocumentsChain**\n",
    "9. **StuffDocumentsChain**\n",
    "10. **QAWithSourcesChain**\n",
    "11. **AgentExecutor**\n",
    "12. **PlanAndExecute**\n",
    "13. **APIChain**\n",
    "14. **LLMSummarizationChain**\n",
    "15. **RouterChain**\n",
    "16. **MultiPromptChain**\n",
    "17. **TransformChain**\n",
    "18. **HypotheticalDocumentEmbedder (HyDE) Chain**\n",
    "19. **ReAct Chain**\n",
    "20. **PALChain (Program-Aided Language)**\n",
    "\n",
    "Let me know if you'd like to filter these by use case (e.g., QA, summarization, tool use).\n",
    "\n",
    "\n",
    "\n",
    "Hereâ€™s a list of different **retrieval techniques for vectors from a vector database** (used in RAG and search systems):\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Basic Vector Retrieval Techniques**\n",
    "\n",
    "1. **K-Nearest Neighbors (KNN)**\n",
    "2. **Approximate Nearest Neighbors (ANN)**\n",
    "3. **Maximal Marginal Relevance (MMR)**\n",
    "4. **Filtered Retrieval (Metadata Filtering)**\n",
    "5. **Hybrid Search (Dense + Sparse/BM25)**\n",
    "6. **Semantic Score Thresholding**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **Advanced & Enhanced Techniques**\n",
    "\n",
    "7. **Re-ranking with Cross-Encoders**\n",
    "8. **Self-query Retrieval (LLM generates structured queries)**\n",
    "9. **Multi-vector per Document (e.g., ColBERT)**\n",
    "10. **Chunk Re-weighting or Boosting**\n",
    "11. **Query Expansion or Reformulation**\n",
    "12. **Time-aware or Context-aware Retrieval**\n",
    "13. **Prompt-as-Retriever (PaR)**\n",
    "14. **Multi-modal Retrieval (e.g., image + text)**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ **System-level Retrieval Architectures**\n",
    "\n",
    "15. **Two-stage Retrieval (coarse â†’ fine)**\n",
    "16. **Retrieval Caching or Locality Optimization**\n",
    "17. **Memory-Augmented Retrieval**\n",
    "18. **Graph-based or Relational Retrieval**\n",
    "19. **Streaming or Incremental Retrieval**\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want code samples or examples for any of these!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7adf6a",
   "metadata": {},
   "source": [
    "##### Context Precision\n",
    "##### Context Recall\n",
    "##### Faithfulness\n",
    "##### Groundedness\n",
    "##### ans relevency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259a459",
   "metadata": {},
   "source": [
    "Great â€” you're referring to the **core evaluation metrics used in [RAGAS](https://ragas.io)** (Retrieval-Augmented Generation Assessment Suite), which are designed specifically for **end-to-end evaluation of RAG pipelines**, including **retrieval quality, answer quality, and factual consistency**.\n",
    "\n",
    "Hereâ€™s a **detailed explanation** of the five main RAGAS metrics:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. **Context Precision**\n",
    "\n",
    "> Measures how much of the retrieved context is relevant to the question.\n",
    "\n",
    "* **Goal**: Evaluate how much of the retrieved text (context) is actually helpful for answering the query.\n",
    "* **High precision** means most of the retrieved chunks are relevant.\n",
    "* **Formula (conceptual)**:\n",
    "\n",
    "  $$\n",
    "  \\text{Context Precision} = \\frac{\\text{Relevant Chunks Retrieved}}{\\text{Total Chunks Retrieved}}\n",
    "  $$\n",
    "\n",
    "âœ… **Good when** your retriever isn't pulling in unnecessary or noisy content.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. **Context Recall**\n",
    "\n",
    "> Measures how much of the relevant context was actually retrieved.\n",
    "\n",
    "* **Goal**: Ensure the system retrieves **enough** of the important content from the corpus to answer the query.\n",
    "* **High recall** means few important documents are missed.\n",
    "* **Formula (conceptual)**:\n",
    "\n",
    "  $$\n",
    "  \\text{Context Recall} = \\frac{\\text{Relevant Chunks Retrieved}}{\\text{All Relevant Chunks in Corpus}}\n",
    "  $$\n",
    "\n",
    "âœ… **Good when** your retriever is not missing useful information hidden deeper in the knowledge base.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. **Faithfulness**\n",
    "\n",
    "> Measures whether the answer generated is **logically supported** by the retrieved context.\n",
    "\n",
    "* **Goal**: Check if the LLM \"hallucinated\" or made up facts not found in the source documents.\n",
    "* Itâ€™s a **factual consistency** check.\n",
    "* RAGAS uses LLMs to assess if every statement in the answer is **faithful** to the context.\n",
    "\n",
    "âœ… **High faithfulness** = Answer does not contain unsupported claims.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. **Groundedness**\n",
    "\n",
    "> Measures whether each sentence in the answer is **explicitly grounded** in the retrieved context.\n",
    "\n",
    "* Unlike faithfulness (which checks factual alignment), groundedness checks whether the **answer can be directly traced** to specific parts of the context.\n",
    "* Grounding is a **stronger requirement** than faithfulness.\n",
    "\n",
    "âœ… **High groundedness** = You can directly match answer sentences to the source.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. **Answer Relevancy**\n",
    "\n",
    "> Measures whether the generated answer is **relevant to the original user query**.\n",
    "\n",
    "* Even if the answer is grounded and faithful, it might still be **off-topic** or **not directly answering the question**.\n",
    "* This metric uses LLM-based scoring to determine **query-answer alignment**.\n",
    "\n",
    "âœ… **High relevance** = The answer actually addresses what the user asked.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary Table\n",
    "\n",
    "| Metric            | Focus                          | Checks for                      |\n",
    "| ----------------- | ------------------------------ | ------------------------------- |\n",
    "| Context Precision | Relevance of retrieved docs    | No extra/noisy chunks           |\n",
    "| Context Recall    | Completeness of retrieval      | No missing key info             |\n",
    "| Faithfulness      | Factual correctness            | No hallucinations               |\n",
    "| Groundedness      | Direct traceability to context | Sentence-to-source alignment    |\n",
    "| Answer Relevancy  | Topical relevance              | Directly answers the user query |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a code example showing how to run these with RAGAS on your own RAG pipeline?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
